{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Extract values in GHz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3.5 GHz']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# Define the pattern\n",
    "pattern_7 = r'[0-9]+\\.[0-9]+\\s*GHz'\n",
    "\n",
    "# Example string containing GHz information\n",
    "example_string = \"The bandwidth is 3.5 GHz in this device.\"\n",
    "\n",
    "# Using regular expression to find the GHz information\n",
    "match = re.findall(pattern_7, example_string)\n",
    "print(match)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Extract Emails*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example@domain.com.', 'info@subdomain.com']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text = \"My email is example@domain.com. Contact me at another.email+info@subdomain.com\"\n",
    "emails = re.findall(r\"[a-zA-Z0-9.]+@[a-zA-Z0-9.]+\", text)\n",
    "print(emails)  \n",
    "\n",
    "# OR \n",
    "\n",
    "import re\n",
    "\n",
    "text = \"My email is example@domain.com. Contact me at another.email+info@subdomain.com\"\n",
    "emails = re.findall(r\"[^ ]+@[a-zA-Z0-9.]+\", text)\n",
    "print(emails)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uct.ac.za']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "lin = 'From stephen.marquard@uct.ac.za Sat Jan  5 09:14:16 2008'\n",
    "y = re.findall('@([^ ]*)',lin)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To extract \"Rating\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rating: 9.0', 'Rating: 5.5', 'Rating: 2.0']\n",
      "['Rating: 9.0', 'Rating: 5.5', 'Rating: 2.0']\n",
      "['9.0', '5.5', '2.0']\n",
      "['9.0', '5.5', '2.0']\n",
      "['9.0', '5.5', '2.0']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "imdb_review_text = \"\"\"\n",
    "Rating: 9.0\n",
    "Username: movielover123\n",
    "Review: The movie was fantastic! Highly recommend it.\n",
    "\n",
    "Rating: 5.5\n",
    "Username: cinemacritic99\n",
    "Review: Disappointed with the plot. Acting was decent though.\n",
    "\n",
    "Rating: 2.0\n",
    "Username: hater94\n",
    "Review: Absolutely awful. Waste of time .\n",
    "\"\"\"\n",
    "# pattern = r\"([^ ]+)\\.\"\n",
    "\n",
    "# Regular expression pattern to extract review text, rating, and username\n",
    "pattern_1 = r\"Rating:\\s+[0-9]*\\.[0-9]*\"# Rating: 9.0  \n",
    "pattern_2 = r\"Rating.*\"# Rating: 9.0\n",
    "pattern_3 = r\"Rating:\\s+([0-9]*\\.[0-9]*)\"# Just 9.0\n",
    "pattern_4 = r'.*([0-9]+\\.[0-9]+)'\n",
    "\n",
    "\n",
    "match_1 = re.findall(pattern_1, imdb_review_text)\n",
    "match_2 = re.findall(pattern_2, imdb_review_text)\n",
    "match_3 = re.findall(pattern_3, imdb_review_text)\n",
    "match_4 = re.findall(pattern_4, imdb_review_text)\n",
    "\n",
    "print(match_1)\n",
    "print(match_2)\n",
    "print(match_3)\n",
    "print(match_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rating: 9.0', 'Rating: 5.5', 'Rating: 2.0']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "imdb_review_text = \"\"\"\n",
    "Rating: 9.0\n",
    "Username: movielover123\n",
    "Review: The movie was fantastic! Highly recommend it.\n",
    "\n",
    "Rating: 5.5\n",
    "Username: cinemacritic99\n",
    "Review: Disappointed with the plot. Acting was decent though.\n",
    "\n",
    "Rating: 2.0\n",
    "Username: hater94\n",
    "Review: Absolutely awful. Waste of time .\n",
    "\"\"\"\n",
    "pattern = r'.*[0-9]+\\.[0-9]+'\n",
    "\n",
    "# Regular expression pattern to extract review text, rating, and username\n",
    "#pattern = r\"([^ ]+)\\.\"\n",
    "\n",
    "\n",
    "match = re.findall(pattern, imdb_review_text)\n",
    "\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9.0', '5.5', '2.0']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "imdb_review_text = \"\"\"\n",
    "Rating: 9.0\n",
    "Username: movielover123\n",
    "Review: The movie was fantastic! Highly recommend it.\n",
    "\n",
    "Rating: 5.5\n",
    "Username: cinemacritic99\n",
    "Review: Disappointed with the plot. Acting was decent though.\n",
    "\n",
    "Rating: 2.0\n",
    "Username: hater94\n",
    "Review: Absolutely awful. Waste of time .\n",
    "\"\"\"\n",
    "pattern = r'.*(\\d+\\.\\d+)'\n",
    "\n",
    "# Regular expression pattern to extract review text, rating, and username\n",
    "#pattern = r\"([^ ]+)\\.\"\n",
    "\n",
    "\n",
    "match = re.findall(pattern, imdb_review_text)\n",
    "\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9.0', '5.5', '2.0']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "imdb_review_text = \"\"\"\n",
    "Rating: 9.0\n",
    "Username: movielover123\n",
    "Review: The movie was fantastic! Highly recommend it.\n",
    "\n",
    "Rating: 5.5\n",
    "Username: cinemacritic99\n",
    "Review: Disappointed with the plot. Acting was decent though.\n",
    "\n",
    "Rating: 2.0\n",
    "Username: hater94\n",
    "Review: Absolutely awful. Waste of time .\n",
    "\"\"\"\n",
    "pattern = r'[0-9]+\\.[0-9]+'\n",
    "\n",
    "# Regular expression pattern to extract review text, rating, and username\n",
    "#pattern = r\"([^ ]+)\\.\"\n",
    "\n",
    "\n",
    "match = re.findall(pattern, imdb_review_text)\n",
    "\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('9.', '0'), ('movielover12', '3'), ('5.', '5'), ('cinemacritic9', '9'), ('2.', '0'), ('hater9', '4')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "imdb_review_text = \"\"\"\n",
    "Rating: 9.0\n",
    "Username: movielover123\n",
    "Review: The movie was fantastic! Highly recommend it.\n",
    "\n",
    "Rating: 5.5\n",
    "Username: cinemacritic99\n",
    "Review: Disappointed with the plot. Acting was decent though.\n",
    "\n",
    "Rating: 2.0\n",
    "Username: hater94\n",
    "Review: Absolutely awful. Waste of time .\n",
    "\"\"\"\n",
    "pattern = r'([^ ]+)([0-9]+)'\n",
    "\n",
    "# Regular expression pattern to extract review text, rating, and username\n",
    "#pattern = r\"([^ ]+)\\.\"\n",
    "\n",
    "\n",
    "match = re.findall(pattern, imdb_review_text)\n",
    "\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dual band']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "antenna=\"dual band\"\n",
    "\n",
    "pattern = r\"\\bdual\\b[- ]\\bband\\b\"\n",
    "\n",
    "\n",
    "match = re.findall(pattern, antenna)\n",
    "\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on( RegEx) Real-World Example\n",
    "\n",
    "The object of the project is to accurately extract frequency values mentioned in text data using Regular Expressions. Following this extraction, the frequencies are categorized into two groups based on a predefined threshold of 21 GHz: frequencies higher than 21 GHz are labeled as \"high frequency,\" while frequencies less than or equal to 21 GHz are labeled as \"low frequency.\" Finally, the project aims to display these categorized frequencies for further analysis or visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "data=pd.read_csv(\"ABS.csv\")\n",
    "data.info()\n",
    "\n",
    "\n",
    "pattern=r\"([0-9]*\\.[0-9]*)\\s*ghz\"\n",
    "aftre_testing=[]\n",
    "for i in range(0,2644):\n",
    "    new=data[\"Abstract\"][i]\n",
    "    after_lower=new.lower()\n",
    "    find_test=re.findall(pattern,after_lower)\n",
    "    aftre_testing.extend(find_test)\n",
    "high=[]\n",
    "low=[]\n",
    "for h in range(0,2201):\n",
    "    test=aftre_testing[h]\n",
    "    if float(test)>=21:\n",
    "        high.append(\"high\")\n",
    "    else:\n",
    "        low.append(\"low\")\n",
    "    \n",
    "low.extend(high)   \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "count_file=Counter(low) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries:\n",
    "\n",
    "1. numpy as np: NumPy is a library for numerical computations in Python.\n",
    "2. pandas as pd: Pandas is a library commonly used for data manipulation and analysis.\n",
    "3. re: The re module provides support for working with regular expressions in Python.\n",
    "Read the data from the CSV file \"ABS.csv\" into a Pandas DataFrame named data.\n",
    "\n",
    "4. Use the info() method to display information about the DataFrame data, including the data types and number of non-null values in each column.\n",
    "\n",
    "5. Define a regular expression pattern to match frequency values in gigahertz (GHz) within the abstracts of the dataset. This pattern is intended to capture GHz values in the format \"xx.x GHz\".\n",
    "\n",
    "6. Initialize an empty list after_testing to store the GHz values extracted from the abstracts.\n",
    "\n",
    "7. Iterate through each row (abstract) in the DataFrame:\n",
    "\n",
    "- Convert the abstract text to lowercase.\n",
    "- Use re.findall() to find all occurrences of the specified pattern in the abstract.\n",
    "- Extend the after_testing list with the GHz values found in the abstract.\n",
    "- Initialize empty lists high and low to categorize the GHz values as either \"high\" or \"low\" based on a threshold of 21 GHz.\n",
    "\n",
    "8. Iterate through each GHz value in the after_testing list:\n",
    "\n",
    "- If the GHz value is greater than or equal to 21, append \"high\" to the high list.\n",
    "- Otherwise, append \"low\" to the low list.\n",
    "- Combine the low and high lists.\n",
    "\n",
    "9. Import the Counter class from the collections module to count the occurrences of \"high\" and \"low\" labels in the combined list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP-Stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In natural language processing (NLP), stop words are common words that are often filtered out during the text preprocessing phase because they are considered to have little or no significant meaning in the context of a specific task. Stop words are typically the most common words in a language and include words such as \"the\", \"is\", \"and\", \"of\", \"in\", etc.\n",
    "\n",
    "- Here are some key points about stop words in NLP:\n",
    "\n",
    "1. Purpose: The main purpose of removing stop words is to reduce the dimensionality of text data and improve the efficiency and effectiveness of downstream NLP tasks such as text classification, sentiment analysis, and topic modeling.\n",
    "\n",
    "2. Common Examples: Stop words vary depending on the language being analyzed. In English, common stop words include articles (e.g., \"a\", \"an\", \"the\"), conjunctions (e.g., \"and\", \"or\", \"but\"), prepositions (e.g., \"of\", \"in\", \"on\"), and some common verbs (e.g., \"is\", \"are\", \"was\", \"were\").\n",
    "\n",
    "3. Filtering Stop Words: Stop words are typically filtered out using predefined lists of stop words specific to the language being analyzed. These lists can be created manually or obtained from existing libraries or resources.\n",
    "\n",
    "4. Impact on NLP Tasks: Removing stop words can have a significant impact on the results of NLP tasks. In some cases, removing stop words may improve the performance of tasks such as text classification by reducing noise in the data. However, in other cases, such as sentiment analysis or document summarization, retaining stop words may be beneficial for capturing nuances in language.\n",
    "\n",
    "5. Customization: While predefined lists of stop words are commonly used, they may not always be suitable for every NLP task or domain. In such cases, it may be necessary to customize the list of stop words based on the specific requirements of the task or domain.\n",
    "\n",
    "# NLP-Tokenization \n",
    "\n",
    "Tokenization is the process of breaking down a text document or a sequence of characters into smaller units called tokens. These tokens can be individual words, punctuation marks, or other meaningful elements depending on the context and the specific requirements of the task.\n",
    "\n",
    "In natural language processing (NLP), tokenization is a crucial step in preparing text data for analysis. It helps in converting unstructured text into a format that can be easily processed and analyzed by machines.\n",
    "\n",
    "There are various tokenization techniques available, including:\n",
    "\n",
    "Word Tokenization: Dividing the text into individual words or tokens.\n",
    "Sentence Tokenization: Splitting the text into individual sentences.\n",
    "Character Tokenization: Breaking down the text into individual characters.\n",
    "Subword Tokenization: Dividing the text into smaller meaningful units, such as subwords or morphemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = \"This is a sample sentence demonstrating the removal of stop words.\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "filtered_sentence = ' '.join(filtered_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
